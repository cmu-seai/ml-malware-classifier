{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, classification_report, make_scorer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "import string\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "filename = \"preprocessed_data.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data (optional)\n",
    "\n",
    "skip the step if dataset.txt already exists\n",
    "\n",
    "raw data from the Drebin dataset is expected in folder `DATA_DIR`\n",
    "\n",
    "small and medium datasets are contained in this repository. the full dataset can\n",
    "be downloaded from the Drebin webpage\n",
    "\n",
    "in the `DATA_DIR` folder, a file `sha256_family.csv` is expected which indicates the malicious apps\n",
    "and a subdirectory `feature_vectors` is expected which contains a file for every app\n",
    "in the dataset\n",
    "\n",
    "this step reads the raw data and creates a single file in the output path\n",
    "containing the information from all apps\n",
    "\n",
    "the resulting file has two columns separated by tab: first the category of\n",
    "maliciousness (or \"benign\") and second a space-separated list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation completed.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/home/ckaestne/tmp'\n",
    "FAMILY_FILENAME = 'sha256_family.csv'\n",
    "FEATURES = 'feature_vectors'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(path, targetFile):\n",
    "    family_file = os.path.join(path, FAMILY_FILENAME)\n",
    "    feature_path = os.path.join(path, FEATURES)\n",
    "    \n",
    "    # read the list of malicious apps\n",
    "    with open(family_file) as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        next(reader, None)  # skip the headers\n",
    "        malware_dict = {rows[0]: rows[1] for rows in reader}\n",
    "    \n",
    "    # load all the feature files in the DATA_DIR/feature_vectors directory\n",
    "    # write them into single output file\n",
    "    with open(targetFile, mode='w') as dataset:\n",
    "        for file in os.listdir(feature_path):\n",
    "            label = malware_dict.get(file, \"benign\")\n",
    "            feature = extract_feature_naive(os.path.join(feature_path, file))\n",
    "            dataset.write(label + '\\t' + feature + \"\\n\")\n",
    "\n",
    "\n",
    "def extract_feature_naive(filename):\n",
    "    feature = ''\n",
    "    with open(filename, mode='r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            feature = ' '.join([feature, line.replace(\" \", \"_\")])\n",
    "\n",
    "    return feature[1:]\n",
    "\n",
    "\n",
    "generate_data(DATA_DIR, filename)\n",
    "print('Dataset generation completed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data and selecting features\n",
    "\n",
    "the preprocessed data in `filename` contains all the features representing characteristics of the apps.\n",
    "several characteristics include names and URLs, so the space is not limited to a finite set\n",
    "of characteristics.\n",
    "\n",
    "for prediction, it may make sense to select only a subset of these characteristics as features.\n",
    "the code below makes such a selection and translates the data into a big dataframe with one\n",
    "column per selected characteristic. that is, we have a potentially large list of (binary) features.\n",
    "\n",
    "several characteristics are droped (call, activity, url, ...), but could be added back as features\n",
    "again. It's also possible to further preprocess and group characteristics into fewer features or\n",
    "into nonboolean features.\n",
    "\n",
    "the `max_features` parameter of load data restricts the number of features (columns) used\n",
    "to those that occur most frequently across all apps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(x):\n",
    "    # split the long feature string into multiple parts and drop a number of \n",
    "    # extra information that we won't use for modeling (calls, activities, urls)\n",
    "    features = x.split(\" \")\n",
    "    return filter(lambda x: not (x.startswith(\"call\") or x.startswith(\"activity\") or x.startswith(\"url\") or x.startswith(\"provider\") or x.startswith(\"service_receiver\")), features)\n",
    "\n",
    "def load_data(filename, max_features=None):\n",
    "    # create a data frame with pandas\n",
    "    raw_data = pd.read_table(filename, sep='\\t', header=None, names=['label', 'features'])\n",
    "    # convert the labels from strings to binary values\n",
    "    raw_data.label[raw_data.label != 'benign'] = 'malicious'\n",
    "    raw_data = raw_data.dropna()\n",
    "\n",
    "    # transform the data into occurrences,\n",
    "    # which will be the features that we will feed into the model\n",
    "    count_vect = CountVectorizer(analyzer=feature_selection, lowercase=False, max_features=max_features)\n",
    "    counts = count_vect.fit_transform(raw_data['features'])\n",
    "    \n",
    "    # convert resulting matrix into dataframe\n",
    "    df=pd.DataFrame(data=counts.toarray(), columns=count_vect.get_feature_names())\n",
    "    df['label'] = raw_data.label\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "df = load_data(filename)\n",
    "X = df.drop(\"label\", axis=1)\n",
    "y = df[\"label\"]\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple learning code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9542995915663981"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train vs test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# fit model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# accuracy of the model\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning with crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckaestne/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9521317965826002\n",
      "Precision: 0.05802878015702564\n",
      "Recall: 0.004530207713631128\n",
      "F1 score: 0.008147982890696164\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(10, True, 1)\n",
    "s_kfold = StratifiedKFold(10, True, 1)\n",
    "model = MultinomialNB()\n",
    "\n",
    "acc_scores = cross_val_score(model, X, y, cv=s_kfold)\n",
    "print('Accuracy: ' + repr(np.mean(acc_scores)))\n",
    "\n",
    "prec_scores = cross_val_score(model, X, y, scoring=make_scorer(precision_score, pos_label='malicious'), cv=s_kfold)\n",
    "print('Precision: ' + repr(np.mean(prec_scores)))\n",
    "\n",
    "recall_scores = cross_val_score(model, X, y, scoring=make_scorer(recall_score, pos_label='malicious'), cv=s_kfold)\n",
    "print('Recall: ' + repr(np.mean(recall_scores)))\n",
    "\n",
    "f1_scores = cross_val_score(model, X, y, scoring=make_scorer(fbeta_score, beta=1, pos_label='malicious'), cv=s_kfold)\n",
    "print('F1 score: ' + repr(np.mean(f1_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
